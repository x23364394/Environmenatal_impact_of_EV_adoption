{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f19ddb0-1aa7-42cb-9e8f-406f076297f9",
   "metadata": {},
   "source": [
    "# <b><U>9APDV: Analytics Programming & Data Visualization<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127fd6b6-4e00-451c-a791-687a1e60542d",
   "metadata": {},
   "source": [
    "## CA-2 - Group Project[ 70 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb256ec-5e2d-43a0-93b5-6a73b9733259",
   "metadata": {},
   "source": [
    "## 0. Team Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f405056c-b6f6-45ed-93e6-bbd0a6e448a8",
   "metadata": {},
   "source": [
    "1.1. Student Name  : <b>Vishal Chaudhary</b> <br> \n",
    "1.2. Student ID    : <b>23332794</b> <br>\n",
    "1.3. Stduent Email : x23332794@student.ncirl.ie <br>\n",
    "\n",
    "2.1. Student Name  : <b>Murtza Arab </b> <br>\n",
    "2.2. Student ID    : <b>23364394 </b> <br>\n",
    "2.3. Stduent Email : x23364394@student.ncirl.ie  <br>\n",
    "\n",
    "3.1. Student Name  : <b>Danish Rehman </b> <br>\n",
    "3.2. Student ID    : <b>23425857 </b> <br>\n",
    "3.3. Stduent Email : x23425857@student.ncirl.ie  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2193e451-e82a-40ed-8d1f-076b171e55ea",
   "metadata": {},
   "source": [
    "## 1.1 <b>Washington AQI Data Description<b> \n",
    "\n",
    " <font size=\"+0.8\"><b>Features description:</b></font>\n",
    "\"\"\"\n",
    "<ul>\n",
    "    <br><li><b>state_code</b>: Numeric code representing the U.S. state where the monitoring site is located.</li>\n",
    "    <br><li><b>county_code</b>: Numeric code representing the county within the state.</li>\n",
    "    <br><li><b>site_number</b>: Unique identifier for the specific monitoring site.</li>\n",
    "    <br><li><b>parameter_code</b>: Numeric code for the type of pollutant or parameter being measured.</li>\n",
    "    <br><li><b>poc</b>: Parameter Occurrence Code – used to distinguish between multiple instruments measuring the same parameter at a site.</li>\n",
    "    <br><li><b>latitude</b>: Geographic latitude of the monitoring site.</li>\n",
    "    <br><li><b>longitude</b>: Geographic longitude of the monitoring site.</li>\n",
    "    <br><li><b>datum</b>: Geodetic datum used for the site's geographic coordinates (WGS84)</li>\n",
    "    <br><li><b>parameter</b>: \tName of the pollutant or parameter measured \"PM2.5 - Local Conditions\".</li>\n",
    "    <br><li><b>sample_duration_code</b>: Coded value representing the duration of the sample collection.</li>\n",
    "    <br><li><b>sample_duration</b>: Full name/description of the sample duration.</li>\n",
    "    <br><li><b>pollutant_standard</b>: Air quality standard associated with the parameter.</li>\n",
    "    <br><li><b>date_local</b>: Date of observation format: YYYY-MM-DD.</li>\n",
    "    <br><li><b>units_of_measure</b>: Units in which the pollutant concentration is measured.</li>\n",
    "    <br><li><b>event_type</b>: Type of event during sampling.</li>\n",
    "    <br><li><b>observation_count</b>: Number of observations included in the reported value calculation.</li>\n",
    "    <br><li><b>observation_percent</b>: Percentage of observations needed to be measured over the sample period.</li>\n",
    "    <br><li><b>validity_indicator</b>: Indicator flag for whether the data is valid.</li>\n",
    "    <br><li><b>arithmetic_mean</b>: Average concentration value of the pollutant.</li>\n",
    "    <br><li><b>first_max_value</b>: Maximum concentration value observed.</li>\n",
    "    <br><li><b>first_max_hour</b>: Day hour (0–23) when the first_max_value was achieved.</li>\n",
    "    <br><li><b>aqi</b>: Air Quality Index value based upon the observation.</li>\n",
    "    <br><li><b>method_code</b>: Numerical code referring to the method in which the pollutant was measured.</li>\n",
    "    <br><li><b>method</b>: Indicates measurement method used.</li>\n",
    "    <br><li><b>local_site_name</b>: Monitor location name, as provided by the local agency.</li>\n",
    "    <br><li><b>site_address</b>: Physical street address of monitoring site.</li>\n",
    "    <br><li><b>state</b>: Complete title of the U.S. state where the site is located.</li>\n",
    "    <br><li><b>county</b>: Complete title of the county where the site is located.</li>\n",
    "    <br><li><b>city</b>: City/town name where monitoring site is located.</li>\n",
    "    <br><li><b>cbsa_code</b>: Core-Based Statistical Area code — identifies metropolitan/micropolitan divisions.</li>\n",
    "    <br><li><b>cbsa</b>: Core-Based Statistical Area code —CBSA region name .</li>\n",
    "    <br><li><b>date_of_last_change</b>: Date on which the data for this record was last changed.</li>\n",
    "</ul>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e713b4dc-965b-4df3-a040-ee43759e1a06",
   "metadata": {},
   "source": [
    "## 2.1 AQI DATA IMPORT\n",
    "Importing AQI data from United States Environmental Protection Agency through API\n",
    "\n",
    "Data source: https://aqs.epa.gov/aqsweb/documents/data_api.html \n",
    "\n",
    "Sample Url: https://aqs.epa.gov/data/api/dailyData/bySiteemail=test@aqs.api&key=test&param=44201&bdate=20170618&edate=20170618&state=37&county=183&site=0014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d84915b5-58cf-486a-b987-3bbc3d45ffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all required libraries  \n",
    "import requests\n",
    "import json\n",
    "import pymongo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import mysql.connector\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c208a83d-4df6-405c-aef6-b474696952fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environmental protection Agency api credentials that is going to be used to collect data\n",
    "acc_email = \"ch.vishal400@gmail.com\"  \n",
    "acc_apikey = \"sandkit13\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65b3f1ec-228f-4ef2-93c6-ee254e36acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common link to retrieve data by providing required parameters\n",
    "com_url = \"https://aqs.epa.gov/data/api/dailyData/byState\"\n",
    "\n",
    "# parameters that are required by the common link to retrieve data\n",
    "params = {\n",
    "    \"email\": acc_email,\n",
    "    \"key\": acc_apikey,\n",
    "    \"param\": \"88101\",  # PM2.5 (Particulate Matter pollution Local Conditions)\n",
    "    \"bdate\": \"20240101\",  # Dataset Begining date 1 January 2024\n",
    "    \"edate\": \"20241231\",  # Dataset Ending date 31 December 2024\n",
    "    \"state\": \"53\" }  # code of the Washington state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0170b723-e67b-4c02-bdad-77d28e1291dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining function to get data from EPA by using api\n",
    "def air_quality(com_url, params):\n",
    "    response = requests.get(com_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        wa_data = response.json()\n",
    "        if wa_data[\"Header\"][0][\"status\"] == \"Success\":\n",
    "            return wa_data[\"Data\"]\n",
    "        else:\n",
    "            print(\"Some error occured:\", wa_data[\"Header\"][0][\"error\"])\n",
    "            return None\n",
    "    else:\n",
    "        print(f\" Error: {response.status_code}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "92188ebd-26ef-46d3-9b71-71474eef41d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stared collecting data of washington aqi, 2024...\n"
     ]
    }
   ],
   "source": [
    "# retrieving the dataset of Washington aqi from EPA and converting it to dataframe\n",
    "print(\"Stared collecting data of washington aqi, 2024...\")\n",
    "air_quality_data = air_quality(com_url, params)\n",
    "\n",
    "wa_air_quality = pd.DataFrame(air_quality_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f17b333-92ec-4b74-8dd7-f027f8214175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "  state_code county_code site_number parameter_code  poc   latitude  \\\n",
      "0         53         033        0030          88101    5  47.597222   \n",
      "1         53         033        0030          88101    5  47.597222   \n",
      "2         53         033        0030          88101    5  47.597222   \n",
      "3         53         033        0030          88101    5  47.597222   \n",
      "4         53         033        0030          88101    5  47.597222   \n",
      "\n",
      "    longitude  datum                 parameter sample_duration_code  ...  \\\n",
      "0 -122.319722  WGS84  PM2.5 - Local Conditions                    1  ...   \n",
      "1 -122.319722  WGS84  PM2.5 - Local Conditions                    X  ...   \n",
      "2 -122.319722  WGS84  PM2.5 - Local Conditions                    X  ...   \n",
      "3 -122.319722  WGS84  PM2.5 - Local Conditions                    X  ...   \n",
      "4 -122.319722  WGS84  PM2.5 - Local Conditions                    X  ...   \n",
      "\n",
      "  method_code                                             method  \\\n",
      "0         170  Met One BAM-1020 Mass Monitor w/VSCC - Beta At...   \n",
      "1         170  Met One BAM-1020 Mass Monitor w/VSCC - Beta At...   \n",
      "2         170  Met One BAM-1020 Mass Monitor w/VSCC - Beta At...   \n",
      "3         170  Met One BAM-1020 Mass Monitor w/VSCC - Beta At...   \n",
      "4         170  Met One BAM-1020 Mass Monitor w/VSCC - Beta At...   \n",
      "\n",
      "         local_site_name   site_address       state  county     city  \\\n",
      "0  Seattle-10th & Weller  10th & Weller  Washington    King  Seattle   \n",
      "1  Seattle-10th & Weller  10th & Weller  Washington    King  Seattle   \n",
      "2  Seattle-10th & Weller  10th & Weller  Washington    King  Seattle   \n",
      "3  Seattle-10th & Weller  10th & Weller  Washington    King  Seattle   \n",
      "4  Seattle-10th & Weller  10th & Weller  Washington    King  Seattle   \n",
      "\n",
      "  cbsa_code                         cbsa  date_of_last_change  \n",
      "0     42660  Seattle-Tacoma-Bellevue, WA           2025-04-04  \n",
      "1     42660  Seattle-Tacoma-Bellevue, WA           2025-04-04  \n",
      "2     42660  Seattle-Tacoma-Bellevue, WA           2025-04-04  \n",
      "3     42660  Seattle-Tacoma-Bellevue, WA           2025-04-04  \n",
      "4     42660  Seattle-Tacoma-Bellevue, WA           2025-04-04  \n",
      "\n",
      "[5 rows x 32 columns]\n",
      "['state_code', 'county_code', 'site_number', 'parameter_code', 'poc', 'latitude', 'longitude', 'datum', 'parameter', 'sample_duration_code', 'sample_duration', 'pollutant_standard', 'date_local', 'units_of_measure', 'event_type', 'observation_count', 'observation_percent', 'validity_indicator', 'arithmetic_mean', 'first_max_value', 'first_max_hour', 'aqi', 'method_code', 'method', 'local_site_name', 'site_address', 'state', 'county', 'city', 'cbsa_code', 'cbsa', 'date_of_last_change']\n",
      "Number of rows: 70939\n"
     ]
    }
   ],
   "source": [
    "# checking the overall data \n",
    "print(type(wa_air_quality))  \n",
    "print(wa_air_quality.head())  \n",
    "print(wa_air_quality.columns.tolist())\n",
    "print(\"Number of rows:\", len(wa_air_quality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9f567932-628a-4a63-9207-2faed8606b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving this dataset in the json file\n",
    "with open('air_quality_washington.json', 'w') as f:\n",
    "    json.dump(air_quality_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee04eb1-dde1-4266-899e-b684ad8c4cb0",
   "metadata": {},
   "source": [
    "## 1.2 <b>Washington Alternate Fuel Stations Description<b> \n",
    "\n",
    " <font size=\"+0.8\"><b>Features description:</b></font>\n",
    "\"\"\"\n",
    "<ul>\n",
    "    <br><li><b>access_code</b>: Specifies the access level of the station (e.g., public, private).</li>\n",
    "        <br><li><b>access_days</b>: Indicates the days the station is accessible (e.g., Mon-Fri).</li>\n",
    "        <br><li><b>access_detail_cards</b>: Details about access cards or methods for station use (e.g., RFID, app).</li>\n",
    "        <br><li><b>accept_date</b>: Date when the station started accepting a specific fuel type.</li>\n",
    "        <br><li><b>last_co_expected_date</b>: Last date the station was expected to confirm its operational status.</li>\n",
    "        <br><li><b>fuel_type_code</b>: Code representing the type of fuel offered (e.g., ELEC for electric).</li>\n",
    "        <br><li><b>groups_with_id</b>: Identifier for groups or networks the station is part of (e.g., charging network ID).</li>\n",
    "        <br><li><b>maximum_ve_open_date</b>: The latest date a vehicle can access the station based on its operational schedule.</li>\n",
    "        <br><li><b>owner_type</b>: Type of entity that owns the station (e.g., government, private company).</li>\n",
    "        <br><li><b>restricted_access</b>: Indicates if access to the station is restricted (e.g., employees only).</li>\n",
    "        <br><li><b>funding_source</b>: Source of funding for the station (e.g., federal grant, private investment).</li>\n",
    "        <br><li><b>facility_type</b>: Type of facility where the station is located (e.g., gas station, parking lot).</li>\n",
    "        <br><li><b>station_name</b>: Name of the fuel station (e.g., \"Downtown Charging Hub\").</li>\n",
    "        <br><li><b>station_phone</b>: Phone number for contacting the station.</li>\n",
    "        <br><li><b>updated_at</b>: Timestamp of the last update to the station’s data.</li>\n",
    "        <br><li><b>geocode_status</b>: Status of the geocoding process for the station’s location (e.g., 200-9 for success).</li>\n",
    "        <br><li><b>latitude</b>: Geographic latitude coordinate of the station.</li>\n",
    "        <br><li><b>longitude</b>: Geographic longitude coordinate of the station.</li>\n",
    "        <br><li><b>city</b>: City where the station is located (e.g., Seattle).</li>\n",
    "        <br><li><b>country</b>: Country where the station is located (e.g., US).</li>\n",
    "        <br><li><b>intersection_plus4</b>: Additional intersection details for the station’s address (e.g., cross streets).</li>\n",
    "        <br><li><b>state</b>: State where the station is located (e.g., WA).</li>\n",
    "        <br><li><b>street_address</b>: Street address of the station (e.g., 123 Main St).</li>\n",
    "        <br><li><b>zip</b>: ZIP code of the station’s location (e.g., 98101).</li>\n",
    "        <br><li><b>bd_blends</b>: Types of biodiesel blends offered at the station (e.g., B20).</li>\n",
    "        <br><li><b>cng_dispenser</b>: Number or type of compressed natural gas (CNG) dispensers at the station.</li>\n",
    "        <br><li><b>cng_fill_type</b>: Type of CNG fill available (e.g., fast-fill, time-fill).</li>\n",
    "        <br><li><b>cng_psi</b>: Pressure level of CNG offered (e.g., 3600 psi).</li>\n",
    "        <br><li><b>cng_renewable</b>: Indicates if the CNG is sourced from renewable methods (e.g., yes/no).</li>\n",
    "        <br><li><b>cng_total_co</b>: Total compression capacity for CNG at the station.</li>\n",
    "        <br><li><b>cng_total_stc</b>: Total storage capacity for CNG at the station.</li>\n",
    "        <br><li><b>cng_vehicle</b>: Types of vehicles supported by the CNG station (e.g., heavy-duty, light-duty).</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f47ca-30d1-41a5-a239-9d5f8fd819e5",
   "metadata": {},
   "source": [
    "## 2.2 Alternate Fuel Stations DATA IMPORT\n",
    "Importing Alternate Fuel Stations data from United States Environmental Protection Agency through API\n",
    "\n",
    "Data source: https://developer.nrel.gov/api/alt-fuel-stations/v1.json?api_key=8DqksjoYZYBuMOBaVo9ExjXoHmhYSPY3qLd34oHH \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60344ef2-f001-45ba-9662-55a12f087b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file saved as Alt_fuel_stations.json\n"
     ]
    }
   ],
   "source": [
    "#Importing data through API with python REQUESTS module \n",
    "\n",
    "def download_json_from_api(api_url, output_file=\"output.json\"):\n",
    "    try:\n",
    "        # Send GET request to the API\n",
    "        response = requests.get(api_url)\n",
    "        response.raise_for_status()\n",
    "        json_data = response.json()\n",
    "        \n",
    "        # Saving JSON to file\n",
    "        with open(output_file, 'w') as f:\n",
    "            import json\n",
    "            json.dump(json_data, f, indent=4)\n",
    "            \n",
    "        print(f\"JSON file saved as {output_file}\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving file: {e}\")\n",
    "\n",
    "api_url = \"https://developer.nrel.gov/api/alt-fuel-stations/v1.json?api_key=8DqksjoYZYBuMOBaVo9ExjXoHmhYSPY3qLd34oHH\" \n",
    "download_json_from_api(api_url, \"Alt_fuel_stations.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93d3112",
   "metadata": {},
   "source": [
    "## 1.3 <b>Washington EV Vehicle Regisrtions Description<b> \n",
    "\n",
    " <font size=\"+0.8\"><b>Features description:</b></font>\n",
    "\n",
    "<ul>\n",
    "    <br><li><b>Fiscal Year</b>: Descries about the year when the vehicle was purchased.</li>\n",
    "    <br><li><b>Transaction Date</b>: Descries about the exact date when the vehicle was purchased</li>\n",
    "    <br><li><b>Transaction County</b>: Descries about the county where the vehicle was purchased.</li>\n",
    "    <br><li><b>Residential County</b>: Descries about the residentail county of the owner of the vehicle.</li>\n",
    "    <br><li><b>Fuel Type</b>: Descries about the type of fuel vehicle runs on.</li>\n",
    "    <br><li><b>Primary Use Class</b>: Tells us about for what purpose the vehiicle is used in.</li>\n",
    "    \n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521a18f8",
   "metadata": {},
   "source": [
    "## 2.3 Washington EV Vehicle Regisrtion\n",
    "Importing and downloading the json file.\n",
    "\n",
    "https://catalog.data.gov/dataset/vehicle-registrations-by-class-and-county\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c2b89-8ec9-4069-bb55-d7dd3ff57000",
   "metadata": {},
   "source": [
    "## 3.1 Connection With MongoDB and Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b54a0-df5c-48ff-b54c-2d60cf0c616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# import pymongo\n",
    "# from pymongo import MongoClient\n",
    "\n",
    "# # Connect to MongoDB\n",
    "# mongo_cl = MongoClient(\"mongodb://localhost:27017/\")\n",
    "# mongo_db = mongo_cl[\"wa_air_quality\"]\n",
    "# mongo_collection = mongo_db[\"daily_summary\"]\n",
    "\n",
    "# # Load JSON\n",
    "# with open(\"air_quality_washington.json\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# # Print first 2 records to inspect structure\n",
    "# print(data[:2])\n",
    "\n",
    "# #Inserting data in mongo collection\n",
    "# mongo_collection.insert_many(data)\n",
    "# print(\"Data uploaded successfully!\")\n",
    "\n",
    "# # Total number of docs present in collection\n",
    "# count = mongo_collection.count_documents({}) # count docs in collection\n",
    "# print(f\"Total documents: {count}\")\n",
    "\n",
    "# df = mongo_collection.find() # gathering all docs from mongo\n",
    "\n",
    "# mongo_data = pd.DataFrame(list(df)) # converting the data into dataframe\n",
    "\n",
    "# print(mongo_data.dtypes) # checking the datatypes of each column\n",
    "\n",
    "# print(mongo_data.isnull().sum()) # checking if any value is missing in columns\n",
    "\n",
    "# # deleting unwanted columns\n",
    "# required_col = mongo_data.drop(['_id', 'state_code','county_code', 'site_number','parameter_code','first_max_hour', 'method_code','method',\n",
    "#              'poc', 'latitude','longitude', 'datum','parameter','cbsa_code', 'site_address',\n",
    "#              'sample_duration_code', 'sample_duration','pollutant_standard', 'units_of_measure','event_type'], axis = 1)\n",
    "\n",
    "# required_col['cbsa'] = required_col['cbsa'].fillna('Unknown')  #filling missing values in column 'cbsa'\n",
    "# required_col['local_site_name'] = required_col['local_site_name'].fillna('Unknown')  #filling missing values in column 'local_site_name'\n",
    "# required_col['aqi'] = required_col['aqi'].fillna(required_col['arithmetic_mean']) # filling missing values in aqi column with values from mean columns\n",
    "\n",
    "# # convert normal date format to datetime, so that year and month can be extracted\n",
    "# required_col['date_local'] = pd.to_datetime(required_col['date_local'], errors = 'coerce') \n",
    "\n",
    "# print(required_col['date_local'].head()) # check few entries to check the format\n",
    "\n",
    "# required_col['year'] = required_col['date_local'].dt.year #extracted year from date_local column and create a new column 'year'\n",
    "# required_col['month'] = required_col['date_local'].dt.month #extracted month from date_local column and create a new column 'month'\n",
    "# required_col['week'] = required_col['date_local'].dt.isocalendar().week\n",
    "\n",
    "# required_col.head()\n",
    "\n",
    "# # maximum observation count by local aqi, local site name, city and county\n",
    "\n",
    "# obs_count = required_col.groupby(['aqi', 'local_site_name', 'city', 'county', 'date_local'])['observation_count'].max().reset_index()\n",
    "\n",
    "# # Optionally sort it by observation count to see the highest values first\n",
    "# obs_count = grouped.sort_values(by='observation_count', ascending=False)\n",
    "\n",
    "# print(obs_count)\n",
    "\n",
    "# # to get output by the couties, cities, month and week \n",
    "\n",
    "# sorted_required_col = required_col.groupby(['county', 'city', 'month', 'week']).agg({\n",
    "#     'arithmetic_mean': 'mean',\n",
    "#     'first_max_value': 'max',\n",
    "#     'aqi': 'mean',\n",
    "#     'local_site_name': 'nunique',\n",
    "#     'year': 'first'\n",
    "# }).reset_index()\n",
    "\n",
    "# # Renaming the columns according the transformation for clarity\n",
    "# sorted_required_col.rename(columns={'arithmetic_mean':'weekly_pollutant_mean', 'aqi':'weekly_aqi_avg'},inplace=True)\n",
    "\n",
    "# # data sort\n",
    "# sorted_required_col.sort_values(by=['county', 'city', 'month'], inplace=True)\n",
    "\n",
    "# print(sorted_required_col)\n",
    "\n",
    "# # saving data in csv formart\n",
    "# sorted_required_col.to_csv('cleaned_air_quality_washington.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfec689c-be5c-460f-b468-52008a0819c5",
   "metadata": {},
   "source": [
    "## 3.2 Load data to postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5664025-f4ee-4026-890f-5937cdae010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mysql.connector\n",
    "# import numpy as np\n",
    "\n",
    "# df = pd.read_csv('cleaned_air_quality_washington.csv')\n",
    "\n",
    "# print(df.head())\n",
    "\n",
    "# # Connecting to MySQL\n",
    "# mysql_con = mysql.connector.connect(host='localhost', user='aqi_user', password='Vishal123', \n",
    "#                                      database='ev_project_final')\n",
    "\n",
    "# # Created cursor to operate\n",
    "# cursor = mysql_con.cursor()\n",
    "\n",
    "# # Converting all pandas and numpy nullable types to Python types\n",
    "# con_df = df.copy()  \n",
    "# for col in con_df.columns:\n",
    "#     dtype = con_df[col].dtype\n",
    "#     if pd.api.types.is_integer_dtype(dtype):  \n",
    "#         con_df[col] = con_df[col].astype('int32', errors='ignore').fillna(0).apply(lambda x: int(x))  \n",
    "#     elif pd.api.types.is_float_dtype(dtype):  # Handles float64, float32\n",
    "#         con_df[col] = con_df[col].astype('float64', errors='ignore').fillna(0.0).apply(lambda x: float(x))  \n",
    "#     elif pd.api.types.is_string_dtype(dtype) or pd.api.types.is_object_dtype(dtype):\n",
    "#         con_df[col] = con_df[col].astype(str).replace('nan', '') \n",
    "        \n",
    "# # Dropping table if it exists\n",
    "# cursor.execute(\"DROP TABLE IF EXISTS ev_analysis\")\n",
    "\n",
    "# # Creating table with data of DataFrame \n",
    "# columns = con_df.columns\n",
    "# column_dtypes = []\n",
    "# for col in columns:\n",
    "#     dtype = con_df[col].dtype\n",
    "#     if pd.api.types.is_integer_dtype(dtype):\n",
    "#         column_dtypes.append(f\"`{col}` INT\")\n",
    "#     elif pd.api.types.is_float_dtype(dtype):\n",
    "#         column_dtypes.append(f\"`{col}` FLOAT\")\n",
    "#     else:\n",
    "#         column_dtypes.append(f\"`{col}` VARCHAR(255)\")  \n",
    "\n",
    "# # Creating table\n",
    "# create_table = f\"CREATE TABLE ev_analysis ({', '.join(column_dtypes)})\"\n",
    "# cursor.execute(create_table)\n",
    "\n",
    "# # Insert data in the created table \n",
    "# insert_data = f\"INSERT INTO ev_analysis ({', '.join([f'`{col}`' for col in columns])}) VALUES ({', '.join(['%s'] * len(columns))})\"\n",
    "# for row in con_df.itertuples(index=False):\n",
    "#     row = tuple(\n",
    "#         int(x) if pd.api.types.is_integer_dtype(type(x)) \n",
    "#         else float(x) if pd.api.types.is_float_dtype(type(x)) \n",
    "#         else str(x) for x in row\n",
    "#     )\n",
    "#     cursor.execute(insert_data, row)\n",
    "\n",
    "# # Commit changes\n",
    "# mysql_con.commit()\n",
    "\n",
    "# print(\"Cleaned data loaded into MySQL table 'ev_analysis'.\")\n",
    "\n",
    "# # Close cursor and connection\n",
    "# cursor.close()\n",
    "# mysql_con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acacace9",
   "metadata": {},
   "source": [
    "## 3.3 Connection With MongoDB and Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from pymongo import MongoClient\n",
    "\n",
    "# #1:Now we try to upload the json to mongodb\n",
    "# cl=MongoClient('mongodb://localhost:27017/')\n",
    "# dab=cl['ev_project']\n",
    "# coll=dab['ev_registrations']\n",
    "# coll.drop()\n",
    "\n",
    "# with open('ev_registrations.json','r') as file:\n",
    "#     json_da=json.load(file)\n",
    "# da=json_da['data']\n",
    "# colm=[col['name']for col in json_da['meta']['view']['columns']]\n",
    "# data_dt=[dict(zip(colm,recd))for recd in da]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f1288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #it is also necessary for us to insert it in chunks for better efficiency\n",
    "# cz=1000\n",
    "# total_rec=len(data_dt)\n",
    "# for i in range(0,total_rec,cz):\n",
    "#     ck=data_dt[i:i+cz]\n",
    "#     coll.insert_many(ck)\n",
    "# print(f\"Total in MongoDB: {coll.count_documents({})}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1288d47c",
   "metadata": {},
   "source": [
    "Once the data is stored in mongodb run this command in update section of mongodb\n",
    "\n",
    "{\n",
    "  $unset: {\n",
    "    'Transaction County': ''\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0174f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from pymongo import MongoClient\n",
    "# from datetime import datetime\n",
    "\n",
    "# cl=MongoClient('mongodb://localhost:27017/')\n",
    "# dab=cl['ev_project']\n",
    "# coll=dab['ev_registrations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5289a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sd=datetime(2023,1,1)\n",
    "# ed=datetime(2025,6,30)\n",
    "# evda=list(coll.find({'Fuel Type':'Electric','Transaction Date':{'$gte':sd.strftime('%Y-%m-%d'),'$lte':ed.strftime('%Y-%m-%d')}}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd56fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.DataFrame(evda).drop(columns=['_id','sid','id','position','created_at','updated_at','updated_meta','meta','Fiscal Year'],errors='ignore')\n",
    "\n",
    "# df['Counts']=pd.to_numeric(df['Counts'],errors='coerce')\n",
    "# df=df.drop_duplicates()\n",
    "# print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f2a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('uniq residential county',df['Residential County'].nunique())\n",
    "# print('unique fuel type',df['Fuel Type'].nunique())\n",
    "# print(df['Counts'].max())  #we try to see the max value of counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a82ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #we try to print only the monthly data instead of daily data\n",
    "# df['Transaction Date']=pd.to_datetime(df['Transaction Date'])\n",
    "# df['YearMonth']=df['Transaction Date'].dt.to_period('M')\n",
    "# df_group=df.groupby(['Residential County','YearMonth','Fuel Type','Primary Use Class'],as_index=False)['Counts'].sum()\n",
    "# df_group['Residential County']=df_group['Residential County'].str.strip()\n",
    "\n",
    "# print('first 5 rows',df_group.head())\n",
    "# df_group.to_csv('ev_cleaned_monthly.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfae3a95-bf23-4f96-b265-1707cd5ce5f7",
   "metadata": {},
   "source": [
    "## 4. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5762e42c-25d1-4cbc-af4d-876c59dafb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
